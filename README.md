Projeto: RAG Local com LLM via Ollama e ChromaDB ğŸ§ ğŸ’»
Este projeto Ã© um exemplo prÃ¡tico de como criar um RAG (Retrieval-Augmented Generation) rodando totalmente de forma local, usando:

LangChain

Ollama (LLM local - Mistral:7b)

ChromaDB (Vector Store)

Hugging Face Embeddings (MiniLM-L6-v2)

ğŸ“Œ O que este projeto faz?
VocÃª carrega arquivos .txt com conteÃºdo (ex: um capÃ­tulo de livro ğŸ“–), o cÃ³digo transforma esses textos em vetores, armazena numa base vetorial com o ChromaDB, e depois conecta com uma LLM local via Ollama.

Quando vocÃª faz uma pergunta, o sistema busca os trechos mais relevantes no seu banco de dados vetorial, e a LLM gera uma resposta com base nesses documentos.

ğŸ› ï¸ Requisitos
Antes de rodar o projeto, vocÃª precisa ter:

Python 3.10+

Ollama instalado e rodando localmente:
ğŸ‘‰ https://ollama.com/download

Modelos necessÃ¡rios no Ollama:

mistral:7b
VocÃª pode baixar rodando no terminal:

bash
Copiar
Editar
ollama pull mistral:7b
Instalar as bibliotecas Python:

bash
Copiar
Editar
pip install langchain langchain-community langchain-core langchain-huggingface sentence-transformers chromadb
ğŸ“‚ Estrutura bÃ¡sica de pastas:
Copiar
Editar
seu_projeto/
â”œâ”€â”€ app.py
â”œâ”€â”€ arquivo1.txt
â”œâ”€â”€ arquivo2.txt
â””â”€â”€ ...
Coloque seus arquivos .txt na mesma pasta que o app.py.

ğŸš€ Como rodar
No terminal:

bash
Copiar
Editar
python app.py
Depois Ã© sÃ³ fazer suas perguntas!
Exemplo de entrada:

nginx
Copiar
Editar
Digite sua pergunta (ou 'sair' para encerrar):  
Qual a principal estratÃ©gia de Tyrion no capÃ­tulo?
ğŸ§± Como funciona o fluxo:
Carregamento de documentos: LÃª todos os arquivos .txt da pasta.

Split de textos: Divide o conteÃºdo em pedaÃ§os menores (chunks) de atÃ© 500 caracteres.

Embeddings: Transforma esses pedaÃ§os em vetores com o modelo all-MiniLM-L6-v2.

Vector Store: Armazena os vetores no ChromaDB.

LLM Local: Usa o modelo Mistral 7b rodando via Ollama.

Perguntas: O usuÃ¡rio faz perguntas no terminal.
O sistema busca os trechos mais relevantes e a LLM gera a resposta.

ğŸ’¡ ObservaÃ§Ãµes
Como o modelo estÃ¡ rodando localmente, o tempo de resposta pode ser um pouco longo dependendo do hardware.

Este projeto Ã© ideal para quem estÃ¡ explorando RAG local, LLMs open source e vector stores.